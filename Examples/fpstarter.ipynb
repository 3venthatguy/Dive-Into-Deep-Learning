{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e5dfae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a513a74",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mmhpe_train_data.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Here are the cells below the current cell that I am editing in this Jupyter Notebook:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# CELL INDEX: 2\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m data_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmmhpe_train_data.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m labels_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmmhpe_train_labels.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m data_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmmhpe_test_data.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/d2l-env/lib/python3.10/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mmhpe_train_data.npy'"
     ]
    }
   ],
   "source": [
    "data_train = np.load('mmhpe_train_data.npy')\n",
    "labels_train = np.load('mmhpe_train_labels.npy')\n",
    "data_test = np.load('mmhpe_test_data.npy')\n",
    "\n",
    "# Print its structure\n",
    "print(f\"Train Data type: {type(data_train)}\")\n",
    "print(f\"Train Data shape: {data_train.shape}\")\n",
    "\n",
    "print(f\"Test Data type: {type(data_test)}\")\n",
    "print(f\"Test Data shape: {data_test.shape}\")\n",
    "\n",
    "print(f\"Train  labels type: {type(labels_train)}\")\n",
    "print(f\"Train labels shape: {labels_train.shape}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'data_train' is loaded and has shape (:, 60, 32, 32)\n",
    "# We want to visualize 2D images, typically a slice of (32, 32).\n",
    "# Let's plot a few frames from the first sample (index 0).\n",
    "\n",
    "num_frames_to_plot = 6\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(num_frames_to_plot):\n",
    "    # Select the first sample, and the i-th frame\n",
    "    image_data = data_train[0, i, :, :]\n",
    "\n",
    "    plt.subplot(1, num_frames_to_plot, i + 1)\n",
    "    plt.imshow(image_data, cmap='gray') # Assuming grayscale images; adjust cmap if color\n",
    "    plt.title(f'Frame {i+1}')\n",
    "    plt.axis('on')\n",
    "\n",
    "    # Set x and y axis ticks from 0 to 31 with a step of 5\n",
    "    tick_positions = np.arange(0, 32, 5) # Generates [0, 5, 10, 15, 20, 25, 30]\n",
    "    plt.xticks(tick_positions)\n",
    "    plt.yticks(tick_positions)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Let's visualize a few frames from the `data_test` array to understand its structure.\n",
    "\n",
    "# Assuming 'data_test' is loaded and has shape (:, 60, 32, 32)\n",
    "# We want to visualize 2D images, typically a slice of (32, 32).\n",
    "# Let's plot a few frames from the first sample (index 0).\n",
    "\n",
    "num_frames_to_plot = 6\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(num_frames_to_plot):\n",
    "    # Select the first sample, and the i-th frame\n",
    "    image_data = data_test[0, i, :, :]\n",
    "\n",
    "    plt.subplot(1, num_frames_to_plot, i + 1)\n",
    "    plt.imshow(image_data, cmap='gray') # Assuming grayscale images; adjust cmap if color\n",
    "    plt.title(f'Test Frame {i+1}')\n",
    "    plt.axis('on')\n",
    "\n",
    "    # Set x and y axis ticks from 0 to 31 with a step of 5\n",
    "    tick_positions = np.arange(0, 32, 5) # Generates [0, 5, 10, 15, 20, 25, 30]\n",
    "    plt.xticks(tick_positions)\n",
    "    plt.yticks(tick_positions)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "unique_labels, counts = np.unique(labels_train, return_counts=True)\n",
    "print(\"\\nUnique labels and their counts:\")\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f\"Label {label}: {count} occurrences\")\n",
    "\n",
    "out_csv = \"submission_team_Loss_But_Vibing.csv\"\n",
    "seed = 234\n",
    "id_prefix = \"test_\"\n",
    "\n",
    "# --- Generate random labels ---\n",
    "# Get the number of samples from the already loaded data_test\n",
    "n = data_test.shape[0]\n",
    "\n",
    "# Generate 'n' random labels between 1 and 5 (inclusive)\n",
    "rng = np.random.default_rng(seed)\n",
    "pred = rng.integers(1, 6, size=n) # Generate integers from 1 up to (but not including) 6\n",
    "\n",
    "ids = [f\"{id_prefix}{i:04d}\" for i in range(n)]\n",
    "\n",
    "df = pd.DataFrame({\"Id\": ids, \"Label\": pred})\n",
    "df.to_csv(out_csv, index=False)\n",
    "\n",
    "print(f\"Wrote {out_csv} with {n} random labels.\")\n",
    "print(df.head())\n",
    "\n",
    "# Model Design and Temporal Aggregation Strategy\n",
    "# \n",
    "# We design a 2D single-channel convolutional neural network (CNN) that classifies\n",
    "# each 32×32 frame independently into one of five classes.\n",
    "#\n",
    "# For each input sample consisting of 60 frames, the CNN produces a set of\n",
    "# pre-softmax scores (logits) for every frame. These logits are then aggregated\n",
    "# across the temporal dimension to produce a single prediction for the full\n",
    "# 60-frame sequence.\n",
    "#\n",
    "# As a baseline, we average the logits across all 60 frames and apply a softmax\n",
    "# to obtain the final class prediction. We also explore alternative fusion\n",
    "# strategies, such as weighted averaging and max pooling over time.\n",
    "#\n",
    "# We experiment with different CNN architectures, learning rates, and\n",
    "# learning-rate schedules. Batch normalization and dropout are evaluated to\n",
    "# improve generalization.\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class FrameCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN classifier for a SINGLE 32×32 grayscale frame.\n",
    "    Adapted from the demo09b_cnn_classifier example.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),          # 16 × 16 × 16\n",
    "\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)           # 16 × 8 × 8\n",
    "        )\n",
    "\n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * 8 * 8, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = self.fc_block(x)\n",
    "        return x   \n",
    "\n",
    "class SequenceCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies FrameCNN independently to each frame,\n",
    "    then aggregates logits across time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, frame_cnn, aggregation=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.frame_cnn = frame_cnn\n",
    "        self.aggregation = aggregation\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (B, 60, 32, 32)\n",
    "        \"\"\"\n",
    "        B, T, H, W = x.shape\n",
    "\n",
    "        # Treat each frame as an independent image\n",
    "        x = x.view(B * T, 1, H, W)\n",
    "\n",
    "        logits = self.frame_cnn(x)       # (B*T, 5)\n",
    "        logits = logits.view(B, T, -1)   # (B, 60, 5)\n",
    "\n",
    "        # Temporal fusion (pre-softmax)\n",
    "        if self.aggregation == \"mean\":\n",
    "            logits = logits.mean(dim=1)\n",
    "        elif self.aggregation == \"max\":\n",
    "            logits = logits.max(dim=1).values\n",
    "        else:\n",
    "            raise ValueError(\"Unknown aggregation method\")\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Enable GPU support from the demo09b_cnn_classifier example\n",
    "if torch.cuda.is_available():\n",
    "    # first try cuda\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    # if that fails, try mps (apple silicon)\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    # if that fails, use cpu\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device\n",
    "frame_cnn = FrameCNN().to(device)\n",
    "model = SequenceCNN(frame_cnn, aggregation=\"mean\").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# NumPy array from the DataSet\n",
    "class MHPESequenceDataset(Dataset):\n",
    "    def __init__(self, data, labels=None):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        if labels is not None:\n",
    "            self.labels = torch.tensor(labels - 1, dtype=torch.long)\n",
    "        else:\n",
    "            self.labels = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is None:\n",
    "            return self.data[idx]\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "import torch.nn as nn\n",
    "num_classes = 5\n",
    "class BasicFrameNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 16, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(16 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 1, 32, 32)\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)   # logits\n",
    "        return x\n",
    "\n",
    "# DAta Loader\n",
    "\n",
    "full_dataset = MHPESequenceDataset(data_train, labels_train)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_ds, val_ds = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "test_dl  = DataLoader(val_ds, batch_size=16, shuffle=False)\n",
    "\n",
    "frame_net = BasicFrameNet()\n",
    "model = SequenceCNN(frame_net).to(device)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "epochs = 20\n",
    "lrate = 0.0005\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=lrate)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    opt,\n",
    "    max_lr=10 * lrate,\n",
    "    steps_per_epoch=len(train_dl),\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "print('No batch normalization nor dropout nor data augmentation.')\n",
    "\n",
    "basic_tr_accuracy = []\n",
    "basic_ts_accuracy = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # TRAIN\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.train()\n",
    "\n",
    "    for train_iter, data in enumerate(train_dl):\n",
    "        x_batch, y_batch = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        out = model(x_batch)\n",
    "        loss = criterion(out, y_batch)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        scheduler.step()\n",
    "        \n",
    "    basic_tr_accuracy.append(100 * correct / total)\n",
    "\n",
    "    # Test\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dl:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    basic_ts_accuracy.append(100 * correct / total)\n",
    "\n",
    "    print(\n",
    "        'Epoch: {0:2d}   Train Accuracy: {1:.3f}%   Test Accuracy: {2:.3f}%'\n",
    "        .format(epoch + 1,\n",
    "                basic_tr_accuracy[epoch],\n",
    "                basic_ts_accuracy[epoch], epochs=epochs,anneal_strategy='linear')\n",
    "    )\n",
    "\n",
    "print('Done!')\n",
    "\n",
    "model = BasicFrameNet()\n",
    "model.eval()\n",
    "\n",
    "class FrameCNN_BN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FrameCNN_BN,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 1, 32, 32)\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)   # logits\n",
    "\n",
    "        return x\n",
    "\n",
    "class SequenceCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Sequence-level CNN wrapper\n",
    "    - Applies a frame-level CNN to each frame independently\n",
    "    - Aggregates logits across time (mean by default)\n",
    "\n",
    "    Input:  (B, T, H, W)  e.g. (B, 60, 32, 32)\n",
    "    Output: (B, num_classes)\n",
    "    \"\"\"\n",
    "    def __init__(self, frame_cnn, aggregation=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.frame_cnn = frame_cnn\n",
    "        self.aggregation = aggregation\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, H, W)\n",
    "        B, T, H, W = x.shape\n",
    "        # Treat each frame as a grayscale image\n",
    "        x = x.view(B * T, 1, H, W)   # (B*T, 1, 32, 32)\n",
    "        # Frame-level logits\n",
    "        logits = self.frame_cnn(x)  # (B*T, num_classes)\n",
    "        # Restore time dimension\n",
    "        logits = logits.view(B, T, -1)  # (B, T, num_classes)\n",
    "        \n",
    "        # Aggregate across time\n",
    "        if self.aggregation == \"mean\":\n",
    "            logits = logits.mean(dim=1)\n",
    "        elif self.aggregation == \"max\":\n",
    "            logits = logits.max(dim=1).values\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported aggregation type\")\n",
    "            \n",
    "        return logits\n",
    "\n",
    "frame_cnn = FrameCNN_BN().to(device)\n",
    "model = SequenceCNN(frame_cnn, aggregation=\"mean\").to(device)\n",
    "# initiate loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# initiate Adam optimizer\n",
    "opt = optim.Adam(model.parameters(), lr=lrate)\n",
    "\n",
    "# create learning rate scheduler \n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=10*lrate, steps_per_epoch=len(train_dl), epochs=epochs,anneal_strategy='linear')\n",
    "print(str(model))\n",
    "\n",
    "print('Using batch normalization but no dropout or data augmentation.')\n",
    "\n",
    "batch_tr_accuracy = []\n",
    "batch_ts_accuracy = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # TRAIN\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.train()\n",
    "\n",
    "    for train_iter, data in enumerate(train_dl):\n",
    "        x_batch, y_batch = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        out = model(x_batch)\n",
    "        loss = criterion(out, y_batch)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        \n",
    "    scheduler.step()\n",
    "    batch_tr_accuracy.append(100 * correct / total)\n",
    "\n",
    "    # Test\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in test_dl:\n",
    "            images, labels = data[0].to(device),data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    batch_ts_accuracy.append( 100*correct/total )\n",
    "    \n",
    "    # Print details every print_mod epoch\n",
    "    print('Epoch: {0:2d}   Train Accuracy: {1:.3f}%   Test Accuracy: {2:.3f}%'.format(epoch+1, batch_tr_accuracy[epoch], batch_ts_accuracy[epoch]))\n",
    "        \n",
    "\n",
    "print('Done!')\n",
    "\n",
    "class FrameCNN_BN_Dropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FrameCNN_BN_Dropout,self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.bn1   = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(32)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 512)\n",
    "        self.bn3 = nn.BatchNorm1d(512)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.bn1(self.pool(self.relu(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.bn3(self.fc1(x)))    # BN1d AFTER fc\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "frame_cnn_dropout = FrameCNN_BN_Dropout().to(device)\n",
    "model = SequenceCNN(frame_cnn_dropout, aggregation=\"mean\").to(device)\n",
    "\n",
    "# initiate loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# initiate Adam optimizer\n",
    "opt = optim.Adam(model.parameters(), lr=lrate)\n",
    "\n",
    "# create learning rate scheduler \n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=10*lrate, steps_per_epoch=len(train_dl), epochs=epochs,anneal_strategy='linear')\n",
    "print(str(model))\n",
    "\n",
    "print('Using batch normalization and dropout but no data augmentation.')\n",
    "# Fit the model\n",
    "\n",
    "drop_tr_accuracy = []\n",
    "drop_ts_accuracy = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    correct = 0 # initialize error counter\n",
    "    total = 0 # initialize total counter\n",
    "    model.train() # put model in training mode\n",
    "    # iterate over training set\n",
    "    for train_iter, data in enumerate(train_dl):\n",
    "        x_batch,y_batch = data[0].to(device),data[1].to(device)\n",
    "        out = model(x_batch)\n",
    "        # Compute Loss\n",
    "        loss = criterion(out,y_batch)\n",
    "        # Zero gradients\n",
    "        opt.zero_grad()\n",
    "        # Compute gradients using back propagation\n",
    "        loss.backward()\n",
    "        # Take an optimization 'step'\n",
    "        opt.step()\n",
    "        \n",
    "        # Compute Accuracy\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        \n",
    "    # Take scheduler step\n",
    "    scheduler.step()\n",
    "    drop_tr_accuracy.append( 100*correct/total )\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval() # put model in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for data in test_dl:\n",
    "            images, labels = data[0].to(device),data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    drop_ts_accuracy.append( 100*correct/total )\n",
    "    \n",
    "    # Print details every print_mod epoch\n",
    "    print('Epoch: {0:2d}   Train Accuracy: {1:.3f}%   Test Accuracy: {2:.3f}%'.format(epoch+1, drop_tr_accuracy[epoch], drop_ts_accuracy[epoch]))\n",
    "        \n",
    "\n",
    "print('Done!')\n",
    "\n",
    "# submission via kaggle\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x in test_dl:      # test_dl must return ONLY data (no labels)\n",
    "        x = data[0].to(device)\n",
    "        outputs = model(x)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "# Convert back to labels 1–5 if needed\n",
    "all_preds = np.array(all_preds) + 1\n",
    "\n",
    "print(\"Number of predictions:\", len(all_preds))\n",
    "\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"Id\": np.arange(len(all_preds)),\n",
    "    \"Label\": all_preds\n",
    "})\n",
    "\n",
    "submission.to_csv(\"model_neuralnet.csv\", index=False)\n",
    "print(\"Saved model_neuralnet.csv\")\n",
    "\n",
    "test_dataset = MHPESequenceDataset(data_test, labels=None)\n",
    "test_dl_kaggle = DataLoader( test_dataset, batch_size=16, shuffle=False )\n",
    "model.eval()\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x in test_dl_kaggle:   # x is ONLY data\n",
    "        x = x.to(device)\n",
    "        outputs = model(x)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "# Convert back to labels 1–5\n",
    "all_preds = np.array(all_preds) + 1\n",
    "\n",
    "print(\"Number of predictions:\", len(all_preds))\n",
    "\n",
    "ids = [f\"test_{i:04d}\" for i in range(len(all_preds))]\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"Id\": ids,\n",
    "    \"Label\": all_preds\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Saved submission.csv\")\n",
    "\n",
    "df = pd.read_csv(\"submission.csv\")\n",
    "print(df.head())\n",
    "print(df.tail())\n",
    "print(df.shape)\n",
    "print(df[\"Label\"].min(), df[\"Label\"].max())\n",
    "\n",
    "# Team name: Loss But Vibing\n",
    "# Teammate: Micheal Pansari(.02)\n",
    "\n",
    "epochsn=np.arange(1,epochs+1)\n",
    "plt.plot(epochsn,basic_tr_accuracy)\n",
    "plt.plot(epochsn,batch_tr_accuracy)\n",
    "plt.plot(epochsn,drop_tr_accuracy)\n",
    "plt.xlim((1,epochs))\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuarcy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend(['Baseline', 'BN', 'BN+Dropout']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1c05a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
