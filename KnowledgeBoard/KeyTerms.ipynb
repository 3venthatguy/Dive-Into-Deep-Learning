{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "886b2616",
   "metadata": {},
   "source": [
    "### <span style='color:yellow'>Terms</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce3a28b",
   "metadata": {},
   "source": [
    "RNN: processes sequenctial data by maintaining internal hidden state that gets updated as it process each element in a sequence\n",
    "- Hidden state: the internal memory that captures and maintains a summary of previous steps\n",
    "- Hidden states have information decay and gradient tproblems due to the need to summarize past results\n",
    "- Transformers have proved as a better alternative with self-attention allowing every position to the weighted against every other position, even in sequential data\n",
    "\n",
    "CNN: process grid-like data exploiting spatial structue through local connectivity patterns and weight sharing\n",
    "\n",
    "LSTM (long-term short memory)\n",
    "- Addresses vanishing gradients in vanilla RNNs with gated memory cells\n",
    "- Excels in saving long-range dependencies at sequential tasks\n",
    "\n",
    "GRUs (Gated Recurrent Units): simplifies LSTMs by merging the cell state and hidden state into a single hidden state vector\n",
    "\n",
    "GNN (Graph Neural Networks): designed to operate of data where entities/nodes have relationships/edges between them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4ae908",
   "metadata": {},
   "source": [
    "### <span style='color:yellow'>Data Manipulation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f5a2bd",
   "metadata": {},
   "source": [
    "One-hot encoding: converts catagorical data into a binary matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44daba83",
   "metadata": {},
   "source": [
    "### <span style='color:yellow'>Mathematic Relationships</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a116e1",
   "metadata": {},
   "source": [
    "L2 Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f79004",
   "metadata": {},
   "source": [
    "### <span style='color:yellow'>Training</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9951842",
   "metadata": {},
   "source": [
    "Epoch: one complete pass through the entire training dataset\n",
    "\n",
    "K-fold Cross-Validation: splits data into K-folds. During training, the model uses 1 fold for validation and the rest for training\n",
    "\n",
    "Batch: a subset of training examples processed together in a single forward/backward pass before updateing model parameters\n",
    "\n",
    "Dropout: regularization technique that randomly sets a percentage of neurons outputs during each training set to zero\n",
    "- Prevents overfitting by preventing neurons from forming dependencies on training data patterns\n",
    "\n",
    "Activation Function: fixed, nonlinear transformations applied element-wise to their inputs so that networks can learn complex patterns beyond linear combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411f5dae",
   "metadata": {},
   "source": [
    "### <span style='color:yellow'>Content</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2c53ea",
   "metadata": {},
   "source": [
    "Gradient Vanishing: occurs when gradients become very small as they propagate backward in DNN, effectively preventing layers from learning any information\n",
    "- Solved with ReLU, batch normalization, residual connections, & other weight initialization schemes\n",
    "\n",
    "Gradient Explosion: occurs when gradients become excessively large as they propagate backward, effectively destablizing training\n",
    "\n",
    "Gated Memory Cells: a learnable control mechanism that uses activation functions as components in RNNs to selectively remember, forget, and update information\n",
    "\n",
    "Attention Mechanism: allows NN to selectively focus on relevant parts of the input when producing each output, rather than compressing all information into a fixed-size representation\n",
    "- rather than needed to reference data in a sequence to train, attention mechanisms assign \"attention weight\" vectors for each token on every other token\n",
    "\n",
    "Kernel\n",
    "- CNN: matrix filter of learnable weights\n",
    "- Attention mechanism: function that defines the query-key weighted relationship ɑ(q, k)\n",
    "\n",
    "Covariates: training features\n",
    "\n",
    "Attention Scoring Functions: self-attention functions focused on dot-product relationships rather than distance-based kernels -- proves to be more computationally efficient\n",
    "Supervised Learning: model learns from labeled data\n",
    "\n",
    "Unsupervised Learning: model learns structure and patterns from unlabeled data -- the goal is to discover hidden structure & relationships wihout being told what to look for\n",
    "\n",
    "Reinforcement Learning: an agent learns to make decisions by interactig with an environment, recieving rewards/penalties based on actions to maximize cumulative reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a113f25",
   "metadata": {},
   "source": [
    "### <span style='color:yellow'>Optimization Techniques</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91a6ad1",
   "metadata": {},
   "source": [
    "Gradient Clipping: prevents training instability from gradient explosion by limiting gradient magnitudes during backpropagation\n",
    "- Clip by value / Clip by norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e03c81",
   "metadata": {},
   "source": [
    "### <span style='color:yellow'>Optimizers</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b9ceff0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '∂' (U+2202) (2842398393.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    θ = θ - lr * ∂L/∂θ\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '∂' (U+2202)\n"
     ]
    }
   ],
   "source": [
    "# Vanilla Gradient Descent (SGD)\n",
    "θ = θ - lr * ∂L/∂θ\n",
    "\n",
    "# SGD With Momentum\n",
    "v_t = β * v_{t-1} + ∂L/∂θ\n",
    "θ = θ - lr * v_t\n",
    "\n",
    "# AdaGrad (Adaptive Gradient Algorithm)\n",
    "G_t = G_{t-1} + (∂L/∂θ)²\n",
    "θ = θ - (lr / sqrt(G_t + ε)) * ∂L/∂θ\n",
    "\n",
    "# RMSProp (Root Mean Square Propagation)\n",
    "v_t = β₂ * v_{t-1} + (1 - β₂) * (∂L/∂θ)²\n",
    "θ = θ - (lr / sqrt(v_t + ε)) * ∂L/∂θ\n",
    "\n",
    "# Adam (Adaptive Moment Estimation) = Momentum + RMSProp\n",
    "m_t = β₁ * m_{t-1} + (1 - β₁) * ∂L/∂θ       # First moment (mean)\n",
    "v_t = β₂ * v_{t-1} + (1 - β₂) * (∂L/∂θ)²    # Second moment (variance)\n",
    "m̂_t = m_t / (1 - β₁ᵗ)                       # Bias correction\n",
    "v̂_t = v_t / (1 - β₂ᵗ)                       # Bias correction\n",
    "θ = θ - lr * m̂_t / (sqrt(v̂_t) + ε)          # Parameter update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df705e7b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
