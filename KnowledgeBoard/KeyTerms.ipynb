{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "886b2616",
   "metadata": {},
   "source": [
    "### <span style='color:yellow'>Terms</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce3a28b",
   "metadata": {},
   "source": [
    "RNN: processes sequenctial data by maintaining internal hidden state that gets updated as it process each element in a sequence\n",
    "- Hidden state: the internal memory that captures and maintains a summary of previous steps\n",
    "- Hidden states have information decay and gradient tproblems due to the need to summarize past results\n",
    "- Transformers have proved as a better alternative with self-attention allowing every position to the weighted against every other position, even in sequential data\n",
    "\n",
    "CNN: process grid-like data exploiting spatial structue through local connectivity patterns and weight sharing\n",
    "\n",
    "LSTM (long-term short memory)\n",
    "- Addresses vanishing gradients in vanilla RNNs with gated memory cells\n",
    "- Excels in saving long-range dependencies at sequential tasks\n",
    "\n",
    "GRUs (Gated Recurrent Units): simplifies LSTMs by merging the cell state and hidden state into a single hidden state vector\n",
    "\n",
    "GNN (Graph Neural Networks): designed to operate of data where entities/nodes have relationships/edges between them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4ae908",
   "metadata": {},
   "source": [
    "### <span style='color:yellow'>Data Manipulation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f5a2bd",
   "metadata": {},
   "source": [
    "One-hot encoding: converts catagorical data into a binary matrix"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< Updated upstream
   "id": "923e5b67",
   "metadata": {},
   "source": [
    "### Programming"
=======
   "id": "44daba83",
   "metadata": {},
   "source": [
    "### <span style='color:yellow'>Mathematic Relationships</span>"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< Updated upstream
   "id": "ce40a92d",
   "metadata": {},
   "source": [
    "MaxPooling -- downsampling operation reducing spacial dimensions of feature map by extracting the maximum value\n",
    "- nn.MaxPooling(kernel_size=matrix size of extraction, stride=how far to move the window, padding, dilation=space between kernels)\n",
    "\n",
    "Flatten -- converts multidimensional arrays into a single array: 7x7 → nn.Flatten() → 1x49\n",
    "\n",
    "model = nn.Sequenctial() -- chains all sequences which is already imbedded with a feedforward system, as long as all the nn.Module functions are provided. The following rules are provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5682852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✗ Skip connections (ResNet-style)\n",
    "class ResidualBlock(nn.Module):\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x + residual  # ← Can't do this in Sequential!\n",
    "        return x\n",
    "\n",
    "# ✗ Multiple inputs/outputs\n",
    "class MultiPath(nn.Module):\n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.path1(x1)\n",
    "        out2 = self.path2(x2)\n",
    "        return out1, out2  # ← Sequential only handles single input→output\n",
    "\n",
    "# ✗ Conditional logic\n",
    "class Conditional(nn.Module):\n",
    "    def forward(self, x, training):\n",
    "        x = self.conv(x)\n",
    "        if training:  # ← Can't have if-statements in Sequential\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# ✗ Attention mechanisms (query, key, value from same input)\n",
    "class Attention(nn.Module):\n",
    "    def forward(self, x):\n",
    "        q = self.query_proj(x)\n",
    "        k = self.key_proj(x)    # ← Three branches from same input\n",
    "        v = self.value_proj(x)  # Can't represent in Sequential\n",
    "        return attention(q, k, v)\n",
    "\n",
    "# ✓ Custom layers that fit the single-input, single-output pattern\n",
    "class ComplexModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Can use Sequential for sub-components\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(50, 100),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.skip_proj = nn.Linear(100, 100)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        skip = self.skip_proj(identity)\n",
    "        return x + skip  # ← Custom logic in forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a8fc0a",
   "metadata": {},
   "source": [
    "BatchNorm -- normalizes all features to have μ=0 & σ²=1 -- nn.BatchNorm2d/1d(num_features)\n",
    "\n",
    "XGBoost -- tree-based ensemble method "
=======
   "id": "04a116e1",
   "metadata": {},
   "source": [
    "L2 Normalization"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f79004",
   "metadata": {},
   "source": [
    "### <span style='color:yellow'>Training</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9951842",
   "metadata": {},
   "source": [
    "Epoch: one complete pass through the entire training dataset\n",
    "\n",
    "K-fold Cross-Validation: splits data into K-folds. During training, the model uses 1 fold for validation and the rest for training\n",
    "\n",
    "Batch: a subset of training examples processed together in a single forward/backward pass before updateing model parameters\n",
    "\n",
    "Dropout: regularization technique that randomly sets a percentage of neurons outputs during each training set to zero\n",
    "- Prevents overfitting by preventing neurons from forming dependencies on training data patterns\n",
    "\n",
    "Activation Function: fixed, nonlinear transformations applied element-wise to their inputs so that networks can learn complex patterns beyond linear combinations"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< Updated upstream
   "id": "2a113f25",
   "metadata": {},
   "source": [
    "### Optimization Adressing\n"
=======
   "id": "411f5dae",
   "metadata": {},
   "source": [
    "### <span style='color:yellow'>Content</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2c53ea",
   "metadata": {},
   "source": [
    "Gradient Vanishing: occurs when gradients become very small as they propagate backward in DNN, effectively preventing layers from learning any information\n",
    "- Solved with ReLU, batch normalization, residual connections, & other weight initialization schemes\n",
    "\n",
    "Gradient Explosion: occurs when gradients become excessively large as they propagate backward, effectively destablizing training\n",
    "\n",
    "Gated Memory Cells: a learnable control mechanism that uses activation functions as components in RNNs to selectively remember, forget, and update information\n",
    "\n",
    "Attention Mechanism: allows NN to selectively focus on relevant parts of the input when producing each output, rather than compressing all information into a fixed-size representation\n",
    "- rather than needed to reference data in a sequence to train, attention mechanisms assign \"attention weight\" vectors for each token on every other token\n",
    "\n",
    "Kernel\n",
    "- CNN: matrix filter of learnable weights\n",
    "- Attention mechanism: function that defines the query-key weighted relationship ɑ(q, k)\n",
    "\n",
    "Covariates: training features\n",
    "\n",
    "Attention Scoring Functions: self-attention functions focused on dot-product relationships rather than distance-based kernels -- proves to be more computationally efficient\n",
    "Supervised Learning: \n",
    "\n",
    "Unsupervised Learning: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a113f25",
   "metadata": {},
   "source": [
    "### <span style='color:yellow'>Optimization Techniques</span>"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91a6ad1",
   "metadata": {},
   "source": [
    "Gradient Clipping: prevents training instability from gradient explosion by limiting gradient magnitudes during backpropagation\n",
    "- Clip by value / Clip by norm\n",
    "\n",
    "Gradient Vanishing: occurs when gradients become very small as they propagate backward in DNN, effectively preventing layers from learning any information\n",
    "- Solved with ReLU, batch normalization, residual connections, & other weight initialization schemes\n",
    "\n",
    "Gradient Explosion: occurs when gradients become excessively large as they propagate backward, effectively destablizing training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e03c81",
   "metadata": {},
   "source": [
    "### <span style='color:yellow'>Optimizers</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b9ceff0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '∂' (U+2202) (2842398393.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    θ = θ - lr * ∂L/∂θ\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '∂' (U+2202)\n"
     ]
    }
   ],
   "source": [
    "# Vanilla Gradient Descent (SGD)\n",
    "θ = θ - lr * ∂L/∂θ\n",
    "\n",
    "# SGD With Momentum\n",
    "v_t = β * v_{t-1} + ∂L/∂θ\n",
    "θ = θ - lr * v_t\n",
    "\n",
    "# AdaGrad (Adaptive Gradient Algorithm)\n",
    "G_t = G_{t-1} + (∂L/∂θ)²\n",
    "θ = θ - (lr / sqrt(G_t + ε)) * ∂L/∂θ\n",
    "\n",
    "# RMSProp (Root Mean Square Propagation)\n",
    "v_t = β₂ * v_{t-1} + (1 - β₂) * (∂L/∂θ)²\n",
    "θ = θ - (lr / sqrt(v_t + ε)) * ∂L/∂θ\n",
    "\n",
    "# Adam (Adaptive Moment Estimation) = Momentum + RMSProp\n",
    "m_t = β₁ * m_{t-1} + (1 - β₁) * ∂L/∂θ       # First moment (mean)\n",
    "v_t = β₂ * v_{t-1} + (1 - β₂) * (∂L/∂θ)²    # Second moment (variance)\n",
    "m̂_t = m_t / (1 - β₁ᵗ)                       # Bias correction\n",
    "v̂_t = v_t / (1 - β₂ᵗ)                       # Bias correction\n",
    "θ = θ - lr * m̂_t / (sqrt(v̂_t) + ε)          # Parameter update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df705e7b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< Updated upstream
   "display_name": "d2l-env",
=======
   "display_name": "d2l",
>>>>>>> Stashed changes
   "language": "python",
   "name": "python3"
  },
  "language_info": {
<<<<<<< Updated upstream
   "name": "python",
   "version": "3.10.18"
=======
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
