{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "886b2616",
   "metadata": {},
   "source": [
    "### Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce3a28b",
   "metadata": {},
   "source": [
    "RNN: processes sequenctial data by maintaining internal hidden state that gets updated as it process each element in a sequence\n",
    "- Hidden state: the internal memory that captures and maintains a summary of previous steps\n",
    "- Hidden states have information decay and gradient tproblems due to the need to summarize past results\n",
    "- Transformers have proved as a better alternative with self-attention allowing every position to the weighted against every other position, even in sequential data\n",
    "\n",
    "CNN: process grid-like data exploiting spatial structue through local connectivity patterns and weight sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4ae908",
   "metadata": {},
   "source": [
    "### Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f5a2bd",
   "metadata": {},
   "source": [
    "One-hot encoding: converts catagorical data into a binary matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923e5b67",
   "metadata": {},
   "source": [
    "### Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce40a92d",
   "metadata": {},
   "source": [
    "MaxPooling -- downsampling operation reducing spacial dimensions of feature map by extracting the maximum value\n",
    "- nn.MaxPooling(kernel_size=matrix size of extraction, stride=how far to move the window, padding, dilation=space between kernels)\n",
    "\n",
    "Flatten -- converts multidimensional arrays into a single array: 7x7 → nn.Flatten() → 1x49\n",
    "\n",
    "model = nn.Sequenctial() -- chains all sequences which is already imbedded with a feedforward system, as long as all the nn.Module functions are provided. The following rules are provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5682852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✗ Skip connections (ResNet-style)\n",
    "class ResidualBlock(nn.Module):\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x + residual  # ← Can't do this in Sequential!\n",
    "        return x\n",
    "\n",
    "# ✗ Multiple inputs/outputs\n",
    "class MultiPath(nn.Module):\n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.path1(x1)\n",
    "        out2 = self.path2(x2)\n",
    "        return out1, out2  # ← Sequential only handles single input→output\n",
    "\n",
    "# ✗ Conditional logic\n",
    "class Conditional(nn.Module):\n",
    "    def forward(self, x, training):\n",
    "        x = self.conv(x)\n",
    "        if training:  # ← Can't have if-statements in Sequential\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# ✗ Attention mechanisms (query, key, value from same input)\n",
    "class Attention(nn.Module):\n",
    "    def forward(self, x):\n",
    "        q = self.query_proj(x)\n",
    "        k = self.key_proj(x)    # ← Three branches from same input\n",
    "        v = self.value_proj(x)  # Can't represent in Sequential\n",
    "        return attention(q, k, v)\n",
    "\n",
    "# ✓ Custom layers that fit the single-input, single-output pattern\n",
    "class ComplexModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Can use Sequential for sub-components\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(100, 50),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(50, 100),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.skip_proj = nn.Linear(100, 100)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        skip = self.skip_proj(identity)\n",
    "        return x + skip  # ← Custom logic in forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a8fc0a",
   "metadata": {},
   "source": [
    "BatchNorm -- normalizes all features to have μ=0 & σ²=1 -- nn.BatchNorm2d/1d(num_features)\n",
    "\n",
    "XGBoost -- tree-based ensemble method "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f79004",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9951842",
   "metadata": {},
   "source": [
    "Epoch: one complete pass through the entire training dataset\n",
    "\n",
    "K-fold Cross-Validation: splits data into K-folds. During training, the model uses 1 fold for validation and the rest for training\n",
    "\n",
    "Batch: a subset of training examples processed together in a single forward/backward pass before updateing model parameters\n",
    "\n",
    "Activation Funtion: sigmoid, tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a113f25",
   "metadata": {},
   "source": [
    "### Optimization Adressing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91a6ad1",
   "metadata": {},
   "source": [
    "Gradient Clipping: prevents training instability from gradient explosion by limiting gradient magnitudes during backpropagation\n",
    "- Clip by value / Clip by norm\n",
    "\n",
    "Gradient Vanishing: occurs when gradients become very small as they propagate backward in DNN, effectively preventing layers from learning any information\n",
    "- Solved with ReLU, batch normalization, residual connections, & other weight initialization schemes\n",
    "\n",
    "Gradient Explosion: occurs when gradients become excessively large as they propagate backward, effectively destablizing training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e03c81",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
