{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "886b2616",
   "metadata": {},
   "source": [
    "### Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce3a28b",
   "metadata": {},
   "source": [
    "RNN: processes sequenctial data by maintaining internal hidden state that gets updated as it process each element in a sequence\n",
    "- Hidden state: the internal memory that captures and maintains a summary of previous steps\n",
    "- Hidden states have information decay and gradient tproblems due to the need to summarize past results\n",
    "- Transformers have proved as a better alternative with self-attention allowing every position to the weighted against every other position, even in sequential data\n",
    "\n",
    "CNN: process grid-like data exploiting spatial structue through local connectivity patterns and weight sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4ae908",
   "metadata": {},
   "source": [
    "### Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f5a2bd",
   "metadata": {},
   "source": [
    "One-hot encoding: converts catagorical data into a binary matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f79004",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9951842",
   "metadata": {},
   "source": [
    "Epoch: one complete pass through the entire training dataset\n",
    "\n",
    "K-fold Cross-Validation: splits data into K-folds. During training, the model uses 1 fold for validation and the rest for training\n",
    "\n",
    "Batch: a subset of training examples processed together in a single forward/backward pass before updateing model parameters\n",
    "\n",
    "Activation Funtion: sigmoid, tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411f5dae",
   "metadata": {},
   "source": [
    "### Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2c53ea",
   "metadata": {},
   "source": [
    "Gradient Vanishing: occurs when gradients become very small as they propagate backward in DNN, effectively preventing layers from learning any information\n",
    "- Solved with ReLU, batch normalization, residual connections, & other weight initialization schemes\n",
    "\n",
    "Gradient Explosion: occurs when gradients become excessively large as they propagate backward, effectively destablizing training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a113f25",
   "metadata": {},
   "source": [
    "### Optimization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91a6ad1",
   "metadata": {},
   "source": [
    "Gradient Clipping: prevents training instability from gradient explosion by limiting gradient magnitudes during backpropagation\n",
    "- Clip by value / Clip by norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e03c81",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
